name: CI | Run test metrics
on:
  workflow_call:
    inputs:
      tarball-suffix:
        required: false
        type: string
      commit-hash:
        required: false
        type: string
      target-branch:
        required: false
        type: string
        default: ""
      skip:
        required: false
        type: string
        default: no

jobs:
  setup-kata:
    name: Kata Setup
    runs-on: metrics
    env:
      GOPATH: ${{ github.workspace }}
    steps:
      - uses: actions/checkout@v4
        if: ${{ inputs.skip != 'yes' }}
        with:
          ref: ${{ inputs.commit-hash }}
          fetch-depth: 0

      - name: Rebase atop of the latest target branch
        if: ${{ inputs.skip != 'yes' }}
        run: |
          ./tests/git-helper.sh "rebase-atop-of-the-latest-target-branch"
        env:
          TARGET_BRANCH: ${{ inputs.target-branch }}

      - name: get-kata-tarball
        if: ${{ inputs.skip != 'yes' }}
        uses: actions/download-artifact@v4
        with:
          name: kata-static-tarball-amd64${{ inputs.tarball-suffix }}
          path: kata-artifacts

      - name: Install kata
        if: ${{ inputs.skip != 'yes' }}
        run: bash tests/metrics/gha-run.sh install-kata kata-artifacts

      - name: skip
        if: ${{ inputs.skip == 'yes' }}
        run: echo "Skip this test"

  run-metrics:
    needs: setup-kata
    strategy:
      # We can set this to true whenever we're 100% sure that
      # the all the tests are not flaky, otherwise we'll fail
      # all the tests due to a single flaky instance.
      fail-fast: false
      matrix:
        vmm: ['clh', 'qemu', 'stratovirt']
      max-parallel: 1
    runs-on: metrics
    env:
      GOPATH: ${{ github.workspace }}
      KATA_HYPERVISOR: ${{ matrix.vmm }}
    steps:
      - name: enabling the hypervisor
        if: ${{ inputs.skip != 'yes' }}
        run: bash tests/metrics/gha-run.sh enabling-hypervisor

      - name: run launch times test
        if: ${{ inputs.skip != 'yes' }}
        run: bash tests/metrics/gha-run.sh run-test-launchtimes

      - name: run memory foot print test
        if: ${{ inputs.skip != 'yes' }}
        run:  bash tests/metrics/gha-run.sh run-test-memory-usage

      - name: run memory usage inside container test
        if: ${{ inputs.skip != 'yes' }}
        run:  bash tests/metrics/gha-run.sh run-test-memory-usage-inside-container

      - name: run blogbench test
        if: ${{ inputs.skip != 'yes' }}
        run:  bash tests/metrics/gha-run.sh run-test-blogbench

      - name: run tensorflow test
        if: ${{ inputs.skip != 'yes' }}
        run:  bash tests/metrics/gha-run.sh run-test-tensorflow

      - name: run fio test
        if: ${{ inputs.skip != 'yes' }}
        run:  bash tests/metrics/gha-run.sh run-test-fio

      - name: run iperf test
        if: ${{ inputs.skip != 'yes' }}
        run:  bash tests/metrics/gha-run.sh run-test-iperf

      - name: run latency test
        if: ${{ inputs.skip != 'yes' }}
        run:  bash tests/metrics/gha-run.sh run-test-latency

      - name: make metrics tarball ${{ matrix.vmm }}
        if: ${{ inputs.skip != 'yes' }}
        run: bash tests/metrics/gha-run.sh make-tarball-results

      - name: archive metrics results ${{ matrix.vmm }}
        if: ${{ inputs.skip != 'yes' }}
        uses: actions/upload-artifact@v4
        with:
          name: metrics-artifacts-${{ matrix.vmm }}
          path: results-${{ matrix.vmm }}.tar.gz
          retention-days: 1
          if-no-files-found: error

      - name: skip
        if: ${{ inputs.skip == 'yes' }}
        run: echo "Skip this test"
