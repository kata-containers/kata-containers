From 5e0e90706b03fa71072b6b17779e0a66cb14aa64 Mon Sep 17 00:00:00 2001
From: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
Date: Tue, 21 May 2019 15:10:05 +0100
Subject: [PATCH 24/29] DAX: virtiofsd: Rework fs-cache-request error path

Rework error values all the way back to the guest for IO requests.

Signed-off-by: Dr. David Alan Gilbert <dgilbert@redhat.com>
---
 hw/virtio/vhost-user-fs.c                 |  9 +++--
 subprojects/libvhost-user/libvhost-user.c | 18 ++++++----
 subprojects/libvhost-user/libvhost-user.h |  6 ++--
 tools/virtiofsd/fuse_lowlevel.h           | 11 ++++---
 tools/virtiofsd/fuse_virtio.c             | 40 +++++++++++------------
 5 files changed, 45 insertions(+), 39 deletions(-)

diff --git a/hw/virtio/vhost-user-fs.c b/hw/virtio/vhost-user-fs.c
index c02dcaeca7..b43725824f 100644
--- a/hw/virtio/vhost-user-fs.c
+++ b/hw/virtio/vhost-user-fs.c
@@ -283,11 +283,10 @@ uint64_t vhost_user_fs_slave_io(struct vhost_dev *dev, VhostUserFSSlaveMsg *sm,
     close(fd);
 
     trace_vhost_user_fs_slave_io_exit(res, done);
-    /*
-     * TODO! We should be returning 'done' if possible but our error handling
-     * doesn't know about that yet.
-     */
-    return (uint64_t)res;
+    if (res < 0) {
+        return (uint64_t)res;
+    }
+    return (uint64_t)done;
 }
 
 static void vuf_get_config(VirtIODevice *vdev, uint8_t *config)
diff --git a/subprojects/libvhost-user/libvhost-user.c b/subprojects/libvhost-user/libvhost-user.c
index a1cbb626d2..4cf4aef63d 100644
--- a/subprojects/libvhost-user/libvhost-user.c
+++ b/subprojects/libvhost-user/libvhost-user.c
@@ -2919,8 +2919,8 @@ vu_queue_push(VuDev *dev, VuVirtq *vq,
     vu_queue_inflight_post_put(dev, vq, elem->index);
 }
 
-bool vu_fs_cache_request(VuDev *dev, VhostUserSlaveRequest req, int fd,
-                         VhostUserFSSlaveMsg *fsm)
+int64_t vu_fs_cache_request(VuDev *dev, VhostUserSlaveRequest req, int fd,
+                            VhostUserFSSlaveMsg *fsm)
 {
     int fd_num = 0;
     bool res;
@@ -2939,18 +2939,24 @@ bool vu_fs_cache_request(VuDev *dev, VhostUserSlaveRequest req, int fd,
     vmsg.fd_num = fd_num;
 
     if (!vu_has_protocol_feature(dev, VHOST_USER_PROTOCOL_F_SLAVE_SEND_FD)) {
-        return false;
+        return -EINVAL;
     }
 
     pthread_mutex_lock(&dev->slave_mutex);
     if (!vu_message_write(dev, dev->slave_fd, &vmsg)) {
         pthread_mutex_unlock(&dev->slave_mutex);
-        return false;
+        return -EIO;
     }
 
     /* Also unlocks the slave_mutex */
     res = vu_process_message_reply(dev, &vmsg, &payload);
-    res = res && (payload == 0);
-    return res;
+    if (!res) {
+        return -EIO;
+    }
+    /*
+     * Payload is delivered as uint64_t but is actually signed for
+     * errors.
+     */
+    return (int64_t)payload;
 }
 
diff --git a/subprojects/libvhost-user/libvhost-user.h b/subprojects/libvhost-user/libvhost-user.h
index 4b6e681a3e..ee75d4931f 100644
--- a/subprojects/libvhost-user/libvhost-user.h
+++ b/subprojects/libvhost-user/libvhost-user.h
@@ -723,9 +723,9 @@ bool vu_queue_avail_bytes(VuDev *dev, VuVirtq *vq, unsigned int in_bytes,
  * @fd: an fd (only required for map, else must be -1)
  * @fsm: The body of the message
  *
- * Returns: true if the reply was 0
+ * Returns: 0 or above for success, negative errno on error
  */
-bool vu_fs_cache_request(VuDev *dev, VhostUserSlaveRequest req, int fd,
-                         VhostUserFSSlaveMsg *fsm);
+int64_t vu_fs_cache_request(VuDev *dev, VhostUserSlaveRequest req, int fd,
+                            VhostUserFSSlaveMsg *fsm);
 
 #endif /* LIBVHOST_USER_H */
diff --git a/tools/virtiofsd/fuse_lowlevel.h b/tools/virtiofsd/fuse_lowlevel.h
index e543f64177..a36a893871 100644
--- a/tools/virtiofsd/fuse_lowlevel.h
+++ b/tools/virtiofsd/fuse_lowlevel.h
@@ -1998,7 +1998,7 @@ int fuse_session_receive_buf(struct fuse_session *se, struct fuse_buf *buf);
  * @param fd The fd to map
  * @return Zero on success
  */
-int fuse_virtio_map(fuse_req_t req, VhostUserFSSlaveMsg *msg, int fd);
+int64_t fuse_virtio_map(fuse_req_t req, VhostUserFSSlaveMsg *msg, int fd);
 
 /**
  * For use with virtio-fs; request unmapping of part of the cache
@@ -2007,7 +2007,7 @@ int fuse_virtio_map(fuse_req_t req, VhostUserFSSlaveMsg *msg, int fd);
  * @param msg A set of unmapping requests
  * @return Zero on success
  */
-int fuse_virtio_unmap(struct fuse_session *se, VhostUserFSSlaveMsg *msg);
+int64_t fuse_virtio_unmap(struct fuse_session *se, VhostUserFSSlaveMsg *msg);
 
 /**
  * For use with virtio-fs; request synchronisation of part of the cache
@@ -2017,7 +2017,7 @@ int fuse_virtio_unmap(struct fuse_session *se, VhostUserFSSlaveMsg *msg);
  * @param msg A set of syncing requests
  * @return Zero on success
  */
-int fuse_virtio_sync(fuse_req_t req, VhostUserFSSlaveMsg *msg);
+int64_t fuse_virtio_sync(fuse_req_t req, VhostUserFSSlaveMsg *msg);
 
 /**
  * For use with virtio-fs; request IO directly to memory
@@ -2025,9 +2025,10 @@ int fuse_virtio_sync(fuse_req_t req, VhostUserFSSlaveMsg *msg);
  * @param se The current session
  * @param msg A set of IO requests
  * @param fd The fd to map
- * @return Zero on success
+ * @return Length on success, negative errno on error
  */
-int fuse_virtio_io(struct fuse_session *se, VhostUserFSSlaveMsg *msg, int fd);
+int64_t fuse_virtio_io(struct fuse_session *se, VhostUserFSSlaveMsg *msg,
+                       int fd);
 
 /**
  * For use with virtio-fs; wrapper for fuse_virtio_io for writes
diff --git a/tools/virtiofsd/fuse_virtio.c b/tools/virtiofsd/fuse_virtio.c
index 416d285844..9577eaa68d 100644
--- a/tools/virtiofsd/fuse_virtio.c
+++ b/tools/virtiofsd/fuse_virtio.c
@@ -408,13 +408,13 @@ int virtio_send_data_iov(struct fuse_session *se, struct fuse_chan *ch,
             if (len < msg.len[0]) {
                 msg.len[0] = len;
             }
-            bool req_res = !fuse_virtio_io(se, &msg, buf->buf[0].fd);
+            int64_t req_res = fuse_virtio_io(se, &msg, buf->buf[0].fd);
             fuse_log(FUSE_LOG_DEBUG,
                      "%s: bad loop; len=%zd bad_in_num=%d fd_offset=%zd "
-                     "c_offset=%p req_res=%d\n",
+                     "c_offset=%p req_res=%ld\n",
                      __func__, len, bad_in_num, buf->buf[0].pos,
                      in_sg_ptr[0].iov_base, req_res);
-            if (req_res) {
+            if (req_res > 0) {
                 len -= msg.len[0];
                 buf->buf[0].pos += msg.len[0];
                 in_sg_ptr++;
@@ -422,7 +422,7 @@ int virtio_send_data_iov(struct fuse_session *se, struct fuse_chan *ch,
             } else if (req_res == 0) {
                 break;
             } else {
-                ret = EIO;
+                ret = req_res;
                 free(in_sg_cpy);
                 goto err;
             }
@@ -1155,40 +1155,41 @@ void virtio_session_close(struct fuse_session *se)
     se->virtio_dev = NULL;
 }
 
-int fuse_virtio_map(fuse_req_t req, VhostUserFSSlaveMsg *msg, int fd)
+int64_t fuse_virtio_map(fuse_req_t req, VhostUserFSSlaveMsg *msg, int fd)
 {
     if (!req->se->virtio_dev) {
         return -ENODEV;
     }
-    return !vu_fs_cache_request(&req->se->virtio_dev->dev,
-                                VHOST_USER_SLAVE_FS_MAP, fd, msg);
+    return vu_fs_cache_request(&req->se->virtio_dev->dev,
+                               VHOST_USER_SLAVE_FS_MAP, fd, msg);
 }
 
-int fuse_virtio_unmap(struct fuse_session *se, VhostUserFSSlaveMsg *msg)
+int64_t fuse_virtio_unmap(struct fuse_session *se, VhostUserFSSlaveMsg *msg)
 {
     if (!se->virtio_dev) {
         return -ENODEV;
     }
-    return !vu_fs_cache_request(&se->virtio_dev->dev, VHOST_USER_SLAVE_FS_UNMAP,
-                                -1, msg);
+    return vu_fs_cache_request(&se->virtio_dev->dev, VHOST_USER_SLAVE_FS_UNMAP,
+                               -1, msg);
 }
 
-int fuse_virtio_sync(fuse_req_t req, VhostUserFSSlaveMsg *msg)
+int64_t fuse_virtio_sync(fuse_req_t req, VhostUserFSSlaveMsg *msg)
 {
     if (!req->se->virtio_dev) {
         return -ENODEV;
     }
-    return !vu_fs_cache_request(&req->se->virtio_dev->dev,
-                                VHOST_USER_SLAVE_FS_SYNC, -1, msg);
+    return vu_fs_cache_request(&req->se->virtio_dev->dev,
+                               VHOST_USER_SLAVE_FS_SYNC, -1, msg);
 }
 
-int fuse_virtio_io(struct fuse_session *se, VhostUserFSSlaveMsg *msg, int fd)
+int64_t fuse_virtio_io(struct fuse_session *se, VhostUserFSSlaveMsg *msg,
+                       int fd)
 {
     if (!se->virtio_dev) {
         return -ENODEV;
     }
-    return !vu_fs_cache_request(&se->virtio_dev->dev,
-                                VHOST_USER_SLAVE_FS_IO, fd, msg);
+    return vu_fs_cache_request(&se->virtio_dev->dev, VHOST_USER_SLAVE_FS_IO, fd,
+                               msg);
 }
 
 /*
@@ -1214,8 +1215,7 @@ ssize_t fuse_virtio_write(fuse_req_t req, const struct fuse_buf *dst,
     msg.len[0] = len;
     msg.flags[0] = VHOST_USER_FS_FLAG_MAP_W;
 
-    bool result = !fuse_virtio_io(req->se, &msg, dst->fd);
-    /* TODO: Rework the result path to actually get length/error */
-    fuse_log(FUSE_LOG_DEBUG, "%s: result=%d\n", __func__, result);
-    return result ? len : -EIO;
+    int64_t result = fuse_virtio_io(req->se, &msg, dst->fd);
+    fuse_log(FUSE_LOG_DEBUG, "%s: result=%ld\n", __func__, result);
+    return result;
 }
-- 
2.25.1

