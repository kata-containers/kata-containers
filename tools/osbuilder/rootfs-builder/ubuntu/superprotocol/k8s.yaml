apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-cilium
  namespace: kube-system
spec:
  valuesContent: |-
    operator:
      replicas: 1
    cni:
      confPath: /var/lib/cni/net.d

---

apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-coredns
  namespace: kube-system
spec:
  valuesContent: |
    #nodelocal:
    #  enabled: true
    servers:
    - zones:
      - zone: .
      port: 53
      # If serviceType is nodePort you can specify nodePort here
      # nodePort: 30053
      plugins:
      - name: errors
      # Serves a /health endpoint on :8080, required for livenessProbe
      - name: health
        configBlock: |-
          lameduck 5s
      # Serves a /ready endpoint on :8181, required for readinessProbe
      - name: ready
      # Required to query kubernetes API for data
      - name: kubernetes
        parameters: cluster.local in-addr.arpa ip6.arpa
        configBlock: |-
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
          ttl 30
      # Serves a /metrics endpoint on :9153, required for serviceMonitor
      - name: prometheus
        parameters: 0.0.0.0:9153
      - name: forward
        parameters: . /etc/resolv.conf
      - name: cache
        parameters: 30
      - name: loop
      - name: reload
      - name: loadbalance
      - name: hosts
        configBlock: |-
          10.0.2.15  registry.superprotocol.local
          fallthrough

---

apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      hostPort:
        enabled: true
        ports:
          http: 8880
          https: 44443

---

apiVersion: v1
kind: Namespace
metadata:
  name: longhorn-system

---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: longhorn
  namespace: kube-system
spec:
  repo: https://charts.longhorn.io
  chart: longhorn
  version: 1.7.1
  targetNamespace: longhorn-system
  # use only with klipper-helm:0.9.2+
  #oci://hauler.local:5000/hauler/longhorn
  #insecureSkipTLSVerify: true
  #plainHTTP: true
  valuesContent: |-
    #global:
    #  cattle:
    #    systemDefaultRegistry: "hauler.local:5000"
    csi:
      attacherReplicaCount: 1
      provisionerReplicaCount: 1
      resizerReplicaCount: 1
      snapshotterReplicaCount: 1
    persistence:
      defaultClassReplicaCount: 1
    longhornUI:
      replicas: 1
    defaultSettings:
      #defaultDataPath: /data/longhorn
      storageOverProvisioningPercentage: 300 # default 100
      storageMinimalAvailablePercentage: 5   # default 25

---

apiVersion: longhorn.io/v1beta1
kind: RecurringJob
metadata:
  name: filesystem-trim
  namespace: longhorn-system
spec:
  cron: "22 4 * * *"
  task: "filesystem-trim"
  groups:
  - default
  concurrency: 2

---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: sealed-secrets
  namespace: kube-system
spec:
  repo: https://bitnami-labs.github.io/sealed-secrets
  chart: sealed-secrets
  version: 2.16.1
  targetNamespace: kube-system
  valuesContent: |-
    fullnameOverride: sealed-secrets-controller
    image:
      tag: 0.27.1@sha256:6df8cb77885e17dd6cf9b782468504364c0dd03186711a85f1c3b45bc5843a5a

---

apiVersion: v1
kind: Namespace
metadata:
  name: argocd

---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: argocd
  namespace: kube-system
spec:
  #chart: oci://ghcr.io/argoproj/argo-helm/argo-cd:7.6.5
  repo: https://argoproj.github.io/argo-helm
  chart: argo-cd
  version: 7.6.5
  targetNamespace: argocd
  valuesContent: |-
    configs:
      cm:
        ui.bannercontent: "SP-CC-VM"
        ui.bannerurl: ""
        ui.bannerpermanent: "true"
        ui.bannerposition: "both"
    notifications:
      enabled: false
    dex:
      enabled: false
    global:
      image:
        #tag: quay.io/argoproj/argocd:v2.12.4
        tag: v2.12.4
        digest: sha256:4fa3135b836a32c1b366375c6697cddbe279cb6fc315acb11a5395c300f23967
    redis:
      image:
        #tag: public.ecr.aws/docker/library/redis:7.2.4-alpine
        tag: 7.2.4-alpine
        digest: sha256:c8bb255c3559b3e458766db810aa7b3c7af1235b204cfdb304e79ff388fe1a5a

---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: argo-workflows
  namespace: kube-system
spec:
  repo: https://argoproj.github.io/argo-helm
  chart: argo-workflows
  version: 0.42.4
  targetNamespace: argocd
  valuesContent: |-
    workflow:
      serviceAccount:
        create: true
        name: "argo-workflow"
      rbac:
        create: true
    controller:
      image:
        tag: v3.6.0-rc3@sha256:d98a9c2c0062fbf8d92035099215a65be600754c4bec785f42f53b7b123fbacb
      workflowNamespaces:
        - super-protocol
    executor:
      image:
        tag: v3.6.0-rc3@sha256:f5431cdf52db35e03f6979ba60097dae3d354561a8b3645717b72ca191514bd3
    server:
      image:
        tag: v3.6.0-rc3@sha256:b1b4e421430f2fc4d2534b0dd888c3a6fa2329a5abb1e4fe273f673775fa645d
      authModes:
        - server

---

apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager

---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: cert-manager
  namespace: kube-system
spec:
  repo: https://charts.jetstack.io
  chart: cert-manager
  version: v1.15.3
  targetNamespace: cert-manager
  valuesContent: |-
    installCRDs: true
    image: # cert-manager-controller
      tag: v1.15.3@sha256:eee34b3de2dd63f7e5ac459fc2d407662d433fd267d574557b76ee3c7d4bc44f
    cainjector:
      image:
        tag: v1.15.3@sha256:e0ce8ae280c8d7263663b6a6d3ea5e122632936cde9bdd5321cf7109199f51aa
    webhook:
      image:
        tag: v1.15.3@sha256:fdcb9ac4963fa1bb0c9d7cad38f0ba2c65328aa436f8653c998594d936a96488
    acmesolver:
      image:
        tag: v1.15.3@sha256:71468feed486c4cf3ca431d93f996771531ab2e68f261f1a15be845720802a8a

---

apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: self-signed
spec:
  selfSigned: {}

---

apiVersion: v1
kind: Namespace
metadata:
  name: gpu-operator

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config-all
  namespace: gpu-operator
data:
  any: |-
    version: v1
    flags:
      migStrategy: mixed
    sharing:
      timeSlicing:
        renameByDefault: false
        failRequestsGreaterThanOne: false
        resources:
        - name: nvidia.com/gpu
          replicas: 1000
        - name: nvidia.com/mig-1g.12gb
          replicas: 7
        - name: nvidia.com/mig-1g.24gb
          replicas: 4
        - name: nvidia.com/mig-2g.24gb
          replicas: 3
        - name: nvidia.com/mig-3g.47gb
          replicas: 2
        - name: nvidia.com/mig-4g.47gb
          replicas: 1
        - name: nvidia.com/mig-7g.94gb
          replicas: 1
---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: gpu-operator
  namespace: gpu-operator
spec:
  repo: https://nvidia.github.io/gpu-operator
  version: v24.3.0
  chart: gpu-operator
  targetNamespace: gpu-operator
  valuesContent: |-
    driver:
      enabled: false
    #nfd:
    #  nodefeaturerules: true
    devicePlugin:
      config:
        name: time-slicing-config-all
        default: any
    toolkit:
      installDir: "/opt/nvidia"
      env:
      - name: CONTAINERD_CONFIG
        value: /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl
      - name: CONTAINERD_SOCKET
        value: /run/k3s/containerd/containerd.sock
      - name: CONTAINERD_RUNTIME_CLASS
        value: nvidia
      - name: CONTAINERD_SET_AS_DEFAULT
        value: "true"

#---
#
#apiVersion: v1
#kind: Namespace
#metadata:
#  name: logs
#
#---
#
#apiVersion: v1
#kind: Namespace
#metadata:
#  name: monitoring
#
---

##################
# SUPER-PROTOCOL #
##################

apiVersion: v1
kind: Namespace
metadata:
  name: super-protocol

---

apiVersion: v1
kind: Secret
metadata:
  name: regcred
  namespace: super-protocol
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: e30=

---

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sp-root
  namespace: argocd
spec:
  destination:
    server: https://kubernetes.default.svc
  project: default
  source:
    path: argo/clusters/golden
    repoURL: git@github.com:Super-Protocol/sp-golden-image.git
    targetRevision: HEAD
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    retry:
        limit: -1 # unlimited
        backoff:
          duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")
          factor: 2 # a factor to multiply the base duration after each failed retry
          maxDuration: 24h # the maximum amount of time allowed for the backoff strategy
    syncOptions:
      - CreateNamespace=true
      - Replace=true
      - PruneLast=true

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: super-aw-sa
  namespace: argocd

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: super-aw-role
  namespace: argocd
rules:
- apiGroups: [""]
  resources: ["pods", "configmaps", "secrets"]
  verbs: ["create", "delete", "get", "list", "patch", "update", "watch"]
- apiGroups: ["apps", "extensions"]
  resources: ["deployments", "deployments/rollback", "deployments/scale", "statefulsets", "daemonsets"]
  verbs: ["get", "list", "watch", "create", "delete", "update", "patch", "scale"]

- apiGroups: ["argoproj.io"]
  resources: ["applications", "applicationsets", "appprojects"]
  verbs: ["create", "delete", "get", "list", "patch", "update", "watch"]

- apiGroups: ["argoproj.io"]
  resources: ["workflowtaskresults"]
  verbs: ["create", "patch"]
- apiGroups: ["argoproj.io"]
  resources: ["workflowartifactgctasks", "workflowartifactgctasks"]
  verbs: ["list", "watch"]
- apiGroups: ["argoproj.io"]
  resources: ["workflowtasksets/status", "workflowartifactgctasks/status"]
  verbs: ["patch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: super-aw-binding
  namespace: argocd
subjects:
- kind: ServiceAccount
  name: super-aw-sa
  namespace: argocd
roleRef:
  kind: ClusterRole
  name: super-aw-role
  apiGroup: rbac.authorization.k8s.io

---

apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: super-init-manifests
  namespace: argocd
spec:
  schedule: '*/5 * * * *'
  timezone: 'UTC'
  concurrencyPolicy: "Replace"      # Default to "Allow"
  successfulJobsHistoryLimit: 1     # Default 3
  suspend: false                    # Set to "true" to suspend scheduling
  stopStrategy:
    condition: "cronworkflow.succeeded >= 1"     # Run this workflow only once, for first init of provider config
  workflowSpec:
    serviceAccountName: super-aw-sa
    entrypoint: main
    templates:
      - name: main
        dag:
          tasks:
            - name: apply-manifests-from-host
              template: apply-manifests-from-host

      - name: apply-manifests-from-host
        container:
          image: bitnami/kubectl
          command: [sh, -c]
          args:
            - |
              cd /super-protocol-mounted-directory/manifests
              ls -la
              for file in *.yaml; do
                kubectl apply -f "$file"
              done
          volumeMounts:
          - name: sp-config-volume
            mountPath: /super-protocol-mounted-directory

    volumes:
    - name: sp-config-volume
      hostPath:
        path: /super-protocol-mounted-directory
        type: Directory
