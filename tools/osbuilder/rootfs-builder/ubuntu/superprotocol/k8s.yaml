apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-cilium
  namespace: kube-system
spec:
  valuesContent: |-
    operator:
      replicas: 1
    cni:
      confPath: /var/lib/cni/net.d

---

apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-coredns
  namespace: kube-system
spec:
  valuesContent: |
    #nodelocal:
    #  enabled: true
    servers:
    - zones:
      - zone: .
      port: 53
      # If serviceType is nodePort you can specify nodePort here
      # nodePort: 30053
      plugins:
      - name: errors
      # Serves a /health endpoint on :8080, required for livenessProbe
      - name: health
        configBlock: |-
          lameduck 5s
      # Serves a /ready endpoint on :8181, required for readinessProbe
      - name: ready
      # Required to query kubernetes API for data
      - name: kubernetes
        parameters: cluster.local in-addr.arpa ip6.arpa
        configBlock: |-
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
          ttl 30
      # Serves a /metrics endpoint on :9153, required for serviceMonitor
      - name: prometheus
        parameters: 0.0.0.0:9153
      - name: forward
        parameters: . /etc/resolv.conf
      - name: cache
        parameters: 30
      - name: loop
      - name: reload
      - name: loadbalance
      - name: hosts
        configBlock: |-
          10.0.2.15  registry.superprotocol.local
          fallthrough

---

apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      hostPort:
        enabled: true
        ports:
          http: 8880
          https: 44443

---

apiVersion: v1
kind: Namespace
metadata:
  name: longhorn-system

---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: longhorn
  namespace: kube-system
spec:
  repo: https://charts.longhorn.io
  chart: longhorn
  version: 1.7.1
  targetNamespace: longhorn-system
  # use only with klipper-helm:0.9.2+
  #oci://hauler.local:5000/hauler/longhorn
  #insecureSkipTLSVerify: true
  #plainHTTP: true
  valuesContent: |-
    #global:
    #  cattle:
    #    systemDefaultRegistry: "hauler.local:5000"
    csi:
      attacherReplicaCount: 1
      provisionerReplicaCount: 1
      resizerReplicaCount: 1
      snapshotterReplicaCount: 1
    persistence:
      defaultClassReplicaCount: 1
    longhornUI:
      replicas: 1
    defaultSettings:
      #defaultDataPath: /data/longhorn
      storageOverProvisioningPercentage: 300 # default 100
      storageMinimalAvailablePercentage: 5   # default 25

---

apiVersion: longhorn.io/v1beta1
kind: RecurringJob
metadata:
  name: filesystem-trim
  namespace: longhorn-system
spec:
  cron: "22 4 * * *"
  task: "filesystem-trim"
  groups:
  - default
  concurrency: 2

---

apiVersion: v1
kind: Namespace
metadata:
  name: argocd

---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: argocd
  namespace: kube-system
spec:
  #chart: oci://ghcr.io/argoproj/argo-helm/argo-cd:7.6.5
  repo: https://argoproj.github.io/argo-helm
  chart: argo-cd
  version: 7.6.5
  targetNamespace: argocd
  valuesContent: |-
    configs:
      cm:
        ui.bannercontent: "SP-CC-VM"
        ui.bannerurl: ""
        ui.bannerpermanent: "true"
        ui.bannerposition: "both"
    notifications:
      enabled: false
    dex:
      enabled: false
    global:
      image:
        #tag: quay.io/argoproj/argocd:v2.12.4
        tag: v2.12.4
        digest: sha256:4fa3135b836a32c1b366375c6697cddbe279cb6fc315acb11a5395c300f23967
    redis:
      image:
        #tag: public.ecr.aws/docker/library/redis:7.2.4-alpine
        tag: 7.2.4-alpine
        digest: sha256:c8bb255c3559b3e458766db810aa7b3c7af1235b204cfdb304e79ff388fe1a5a

---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: argo-workflows
  namespace: kube-system
spec:
  repo: https://argoproj.github.io/argo-helm
  chart: argo-workflows
  version: 0.42.7
  targetNamespace: argocd
  valuesContent: |-
    workflow:
      serviceAccount:
        create: true
        name: "argo-workflow"
      rbac:
        create: true
    controller:
      image:
        tag: v3.6.0@sha256:75fb7784fb1798ce1a83fff330a49670dc97020d9e436bc6df6b19f920f93425
      workflowNamespaces:
        - super-protocol
    executor:
      image:
        tag: v3.6.0@sha256:cc13a35ec64ccad8fb5a6980502e573f39f4ff28358eca778fe867e62905d053
    server:
      image:
        tag: v3.6.0@sha256:2d9fff5414fa34a3fe56576bbd855233cddcf486081b8232e4f36f64ec0b217c
      authModes:
        - server

---

apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager

---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: cert-manager
  namespace: kube-system
spec:
  repo: https://charts.jetstack.io
  chart: cert-manager
  version: v1.15.3
  targetNamespace: cert-manager
  valuesContent: |-
    installCRDs: true
    image: # cert-manager-controller
      tag: v1.15.3@sha256:eee34b3de2dd63f7e5ac459fc2d407662d433fd267d574557b76ee3c7d4bc44f
    cainjector:
      image:
        tag: v1.15.3@sha256:e0ce8ae280c8d7263663b6a6d3ea5e122632936cde9bdd5321cf7109199f51aa
    webhook:
      image:
        tag: v1.15.3@sha256:fdcb9ac4963fa1bb0c9d7cad38f0ba2c65328aa436f8653c998594d936a96488
    acmesolver:
      image:
        tag: v1.15.3@sha256:71468feed486c4cf3ca431d93f996771531ab2e68f261f1a15be845720802a8a

---

apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: self-signed
spec:
  selfSigned: {}

---

apiVersion: v1
kind: Namespace
metadata:
  name: gpu-operator

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config-all
  namespace: gpu-operator
data:
  any: |-
    version: v1
    flags:
      migStrategy: none
    sharing:
      timeSlicing:
        renameByDefault: false
        failRequestsGreaterThanOne: false
        resources:
        - name: nvidia.com/gpu
          replicas: 1000
---

apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: gpu-operator
  namespace: gpu-operator
spec:
  repo: https://nvidia.github.io/gpu-operator
  version: v24.3.0
  chart: gpu-operator
  targetNamespace: gpu-operator
  valuesContent: |-
    driver:
      enabled: false
    #nfd:
    #  nodefeaturerules: true
    devicePlugin:
      config:
        name: time-slicing-config-all
        default: any
    toolkit:
      installDir: "/opt/nvidia"
      env:
      - name: CONTAINERD_CONFIG
        value: /var/lib/rancher/rke2/agent/etc/containerd/config.toml.tmpl
      - name: CONTAINERD_SOCKET
        value: /run/k3s/containerd/containerd.sock
      - name: CONTAINERD_RUNTIME_CLASS
        value: nvidia
      - name: CONTAINERD_SET_AS_DEFAULT
        value: "true"

#---
#
#apiVersion: v1
#kind: Namespace
#metadata:
#  name: logs
#
#---
#
#apiVersion: v1
#kind: Namespace
#metadata:
#  name: monitoring
#
---

##################
# SUPER-PROTOCOL #
##################

apiVersion: v1
kind: Namespace
metadata:
  name: super-protocol

---

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sp-argo-vm
  namespace: argocd
spec:
  ignoreDifferences:
    - kind: Application
      group: argoproj.io
      jsonPointers:
        - /metadata/annotations/argocd-image-updater.argoproj.io~1mediator.allow-tags
        - /metadata/annotations/argocd-image-updater.argoproj.io~1loader.allow-tags
        - /metadata/annotations/argocd-image-updater.argoproj.io~1ec.allow-tags
        - /metadata/annotations/
  destination:
    server: https://kubernetes.default.svc
  project: default
  source:
    path: argo/clusters/virtual-machine
    repoURL: git@github.com:Super-Protocol/argo-vm.git
    targetRevision: main # argo-vm-selected-branch
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    retry:
        limit: -1 # unlimited
        backoff:
          duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")
          factor: 2 # a factor to multiply the base duration after each failed retry
          maxDuration: 24h # the maximum amount of time allowed for the backoff strategy
    syncOptions:
      - CreateNamespace=true
      - Replace=true
      - PruneLast=true

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: super-aw-sa
  namespace: argocd

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: super-aw-role
  namespace: argocd
rules:
- apiGroups: [""]
  resources: ["pods", "configmaps", "secrets"]
  verbs: ["create", "delete", "get", "list", "patch", "update", "watch"]
- apiGroups: ["apps", "extensions"]
  resources: ["deployments", "deployments/rollback", "deployments/scale", "statefulsets", "daemonsets"]
  verbs: ["get", "list", "watch", "create", "delete", "update", "patch", "scale"]

- apiGroups: ["argoproj.io"]
  resources: ["applications", "applicationsets", "appprojects"]
  verbs: ["create", "delete", "get", "list", "patch", "update", "watch"]

- apiGroups: ["argoproj.io"]
  resources: ["workflowtaskresults"]
  verbs: ["create", "patch"]
- apiGroups: ["argoproj.io"]
  resources: ["workflowartifactgctasks", "workflowartifactgctasks"]
  verbs: ["list", "watch"]
- apiGroups: ["argoproj.io"]
  resources: ["workflowtasksets/status", "workflowartifactgctasks/status"]
  verbs: ["patch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: super-aw-binding
  namespace: argocd
subjects:
- kind: ServiceAccount
  name: super-aw-sa
  namespace: argocd
roleRef:
  kind: ClusterRole
  name: super-aw-role
  apiGroup: rbac.authorization.k8s.io

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: pki-curl-config
  namespace: argocd
data:
  default.conf: |-
    clientCertProvider:
      type: pki-ca
      challenge:
        type: tdx
        idHex: ""
        commonIdHex: ""
      baseUrl: https://ca-subroot1.tee-dev.superprotocol.com:44443/api/v1/pki/
      caBundle: |
        -----BEGIN CERTIFICATE-----
        MIID7TCCAtWgAwIBAgIBATANBgkqhkiG9w0BAQsFADB2MSIwIAYDVQQDExlTdXBl
        clByb3RvY29sIFRFRSBSb290IENBMQswCQYDVQQGEwJVUzELMAkGA1UECBMCTlkx
        ETAPBgNVBAcTCE5ldyBZb3JrMRYwFAYDVQQKEw1TdXBlclByb3RvY29sMQswCQYD
        VQQLEwJJVDAeFw0yNDA4MDEwMDAwMDBaFw0zNDA4MDEwMDAwMDBaMHYxIjAgBgNV
        BAMTGVN1cGVyUHJvdG9jb2wgVEVFIFJvb3QgQ0ExCzAJBgNVBAYTAlVTMQswCQYD
        VQQIEwJOWTERMA8GA1UEBxMITmV3IFlvcmsxFjAUBgNVBAoTDVN1cGVyUHJvdG9j
        b2wxCzAJBgNVBAsTAklUMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
        vyaTmEnNuXjj3m80DLYpQkO2PZhT6C0YdQgLxilyUsRUtQinS88Q2Eof3h0NeqB2
        OMlDMFHRVsIYoi9sdPCINuyYajGy2XUL69al2a2OwiThbZtG6etUEOsBAJmLtnTt
        IDVmzC8p8C+ZMSpu8bZ3AZNeLxHFY2FdZhqYj3FuWgTLwkvERBnJeVMMsSrz99qD
        0hLUDo3iGXw99qsGCLlJC3p+S8NoClc9VjOA7x0+nE9zYrSHqwhwRC+tHdMOBUvd
        5/tPZuDgS3TU++0reo2qnd/O0U59C8XsjP1CrXkz5LDzPSZmMWlaB7hU6edOiUQS
        h8gskt4JNsFCnc0NamCstwIDAQABo4GFMIGCMAwGA1UdEwQFMAMBAf8wCwYDVR0P
        BAQDAgL0MB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEFBQcDAjAdBgNVHQ4EFgQU
        jBSiEoknNMPDd9iKVPz0xmseNF4wJwYDVR0RBCAwHoIcY2EudGVlLWRldi5zdXBl
        cnByb3RvY29sLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEAPBNZ5xm893wUFxKvxbeW
        V8iPL5rOcACIsPzTo7taF8imkb8UQGA6ofpPPZz59YFayNkVV2xKIi9N1x8iI8lg
        szGfsvGm9vvgREl+U5tml1guOxv+DTNAR+vYh91VN/Fg3EoMBomMscQccIHMiuFQ
        C6ru6D0x7aXX8VhYVeDJaJJd+Il/DV3xaFPcMRFhrXLJTBb+JOsnT0kTw5C3RREb
        DVeduduR+4w6Xdz5XUn34iognpSl05ByO8pGQc2NgZIdpyLIIetFPFwITnrFJOV/
        XryT5KVK0PaBSuGAMCCV2wQS9TYVDJwn/Kv7fAJ8Kte4UBBC7vSfEtdGXdT5dcYE
        fw==
        -----END CERTIFICATE-----

---

apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: super-init-manifests
  namespace: argocd
spec:
  concurrencyPolicy: 'Replace'
  schedule: '*/5 * * * *'
  stopStrategy:
    expression: 'cronworkflow.succeeded >= 1'
  successfulJobsHistoryLimit: 1
  suspend: false
  timezone: 'UTC'
  workflowSpec:
    serviceAccountName: super-aw-sa
    entrypoint: main
    templates:
      - name: main
        dag:
          tasks:
            - name: pki-config-challenge
              template: pki-config-challenge

            - name: get-secrets-for-tdx
              template: get-secrets-for-tdx
              depends: pki-config-challenge
              when: "{{tasks.pki-config-challenge.outputs.parameters.challenge-type}} == tdx"

            - name: get-secrets-for-untrusted
              template: get-secrets-for-untrusted
              depends: pki-config-challenge
              when: "{{tasks.pki-config-challenge.outputs.parameters.challenge-type}} == untrusted"

            - name: apply-secrets
              template: apply-secrets
              depends: "get-secrets-for-tdx.Succeeded || get-secrets-for-untrusted.Succeeded"

            - name: force-sync-root-argocd-app
              template: force-sync-root-argocd-app
              depends: "apply-secrets.Succeeded"


      - name: pki-config-challenge
        script:
          # TODO pin tag
          image: bitnami/kubectl
          command: [sh, -c]
          source: |
            set -e
            NAMESPACE="argocd"
            PKI_CONFIGMAP_NAME="pki-curl-config"
            CHALLENGE_FILE="secret.sp-pki-challenge.yaml"
            cd /super-protocol-mounted-directory/manifests

            # Is PKI challenge overridden via custom / developer tokens? 
            if [ -f $CHALLENGE_FILE ]; then
              idHex=$(yq eval '.stringData.idHex' "$CHALLENGE_FILE")
              commonIdHex=$(yq eval '.stringData.commonIdHex' "$CHALLENGE_FILE")
              type=$(yq eval '.stringData.type' "$CHALLENGE_FILE")
              # Is the VM running in debug mode?
              SP_DEBUG=$(grep -oP 'sp-debug=\K\S+' /host-kernel-cmdline)
              if [ $SP_DEBUG = "true" ]; then
                type="untrusted"
              fi
              # update configmap
              filtered_configmap=$(kubectl get configmap "$PKI_CONFIGMAP_NAME" -n $NAMESPACE -o yaml | \
                yq eval "del(.metadata) | .metadata.name = \"$PKI_CONFIGMAP_NAME\"" - | \
                sed -e "s/idHex: .*/idHex: $idHex/" \
                    -e "s/commonIdHex: .*/commonIdHex: $commonIdHex/" \
                    -e "/challenge:/,/^$/s/type: [^ ]*/type: $type/")      
              echo "$filtered_configmap" | kubectl apply -n $NAMESPACE -f -
            fi

            # get challenge and pass it as result output to next steps
            type=$(kubectl get configmap pki-curl-config -n argocd -o jsonpath='{.data.default\.conf}' | yq e '.clientCertProvider.challenge.type' -)
            if [ $type = "tdx" ]; then
              echo "tdx" > /tmp/challenge-type
            else
              echo "untrusted" > /tmp/challenge-type
            fi
          volumeMounts:
            - name: sp-config-volume
              mountPath: /super-protocol-mounted-directory
            - name: host-kernel-cmdline
              mountPath: /host-kernel-cmdline
              readOnly: true
        outputs:
          parameters:
            - name: challenge-type
              valueFrom:
                path: /tmp/challenge-type


      - name: get-secrets-for-tdx
        container:
          image: registry.superprotocol.local:5000/super-protocol/tee-pki-curl:v1.4.4
          command: [sh, -c]
          args:
            - |
              set -e
              SD=/opt/super/deploy
              rm -rf $SD && mkdir -p $SD
              node dist/app.js https://secrets.tee-dev.superprotocol.com:44443/repos/Super-Protocol/tdx-vm-secrets/tarball/main --config /pki-config/default.conf > $SD/secrets_repo.tar.gz
              mkdir -p $SD/secrets_repo && cd $SD/secrets_repo
              tar -xzf $SD/secrets_repo.tar.gz
              inner_dir=$(find ./ -mindepth 1 -maxdepth 1 -type d)
              cd $inner_dir
              cp -r k8s/* $SD/
          volumeMounts:
            - name: sp-deploy
              mountPath: /opt/super
            - name: pki-config
              mountPath: /pki-config
              readOnly: true
            - name: dev-tdx-guest
              mountPath: /dev/tdx_guest
            - name: tdx-attest-conf
              readOnly: true
              mountPath: /etc/tdx-attest.conf
          securityContext:
            privileged: true


      - name: get-secrets-for-untrusted
        container:
          image: registry.superprotocol.local:5000/super-protocol/tee-pki-curl:v1.4.4
          command: [sh, -c]
          args:
            - |
              set -e
              SD=/opt/super/deploy
              rm -rf $SD && mkdir -p $SD
              node dist/app.js https://secrets.tee-dev.superprotocol.com:44443/repos/Super-Protocol/tdx-vm-secrets/tarball/main --config /pki-config/default.conf > $SD/secrets_repo.tar.gz
              mkdir -p $SD/secrets_repo && cd $SD/secrets_repo
              tar -xzf $SD/secrets_repo.tar.gz
              inner_dir=$(find ./ -mindepth 1 -maxdepth 1 -type d)
              cd $inner_dir
              cp -r k8s/* $SD/
          volumeMounts:
            - name: sp-deploy
              mountPath: /opt/super
            - name: pki-config
              mountPath: /pki-config
              readOnly: true


      - name: apply-secrets
        container:
          # TODO pin tag
          image: bitnami/kubectl
          command: [sh, -c]
          args:
            - |
              set -e
              cd /opt/super/deploy
              for file in *.yaml; do
                kubectl apply -f "$file"
              done
              rm -rf /opt/super/deploy/*
          volumeMounts:
            - name: sp-deploy
              mountPath: /opt/super


      - name: force-sync-root-argocd-app
        container:
          image: quay.io/argoproj/argocd:v2.13.1@sha256:19608c266cc41e4986d9b1c2b79ea4c42bb9430269eefc5005e9d65be4d22868
          command: [sh, -c]
          args:
            - |
              set -e
              ARGOCD_APP_NAME=sp-argo-vm
              argocd login argocd-server.argocd.svc:80 --username admin --password $ARGOCD_PASSWORD --insecure
              argocd app sync $ARGOCD_APP_NAME --force
          env:
          - name: ARGOCD_PASSWORD
            valueFrom:
              secretKeyRef:
                name: argocd-initial-admin-secret
                key: password


    volumes:
    - name: sp-config-volume
      hostPath:
        path: /super-protocol-mounted-directory
        type: Directory
    - name: sp-deploy
      hostPath:
        path: /opt/super
        type: Directory
    - name: pki-config
      configMap:
        name: pki-curl-config
    - name: host-kernel-cmdline
      hostPath:
        path: /proc/cmdline
        type: File
    - name: tdx-attest-conf
      hostPath:
        path: /etc/tdx-attest.conf
        type: File
    - name: dev-tdx-guest
      hostPath:
        path: /dev/tdx_guest
        type: ''
